{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tds = Loader.load_imgs(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 64, 64, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vds = Loader.load_imgs(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 64, 64, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factory import ModelFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec = ModelFactory.get_enc_dec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.9375\n",
      "Epoch 00001: val_loss improved from inf to 1.50011, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 95s 475us/sample - loss: 1.9372 - val_loss: 1.5001\n",
      "Epoch 2/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.4413\n",
      "Epoch 00002: val_loss improved from 1.50011 to 1.37751, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 95s 473us/sample - loss: 1.4413 - val_loss: 1.3775\n",
      "Epoch 3/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.3639\n",
      "Epoch 00003: val_loss improved from 1.37751 to 1.33505, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 97s 485us/sample - loss: 1.3639 - val_loss: 1.3351\n",
      "Epoch 4/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.3255\n",
      "Epoch 00004: val_loss improved from 1.33505 to 1.30325, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 95s 474us/sample - loss: 1.3255 - val_loss: 1.3032\n",
      "Epoch 5/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.3061\n",
      "Epoch 00005: val_loss improved from 1.30325 to 1.28945, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 469us/sample - loss: 1.3061 - val_loss: 1.2894\n",
      "Epoch 6/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2937\n",
      "Epoch 00006: val_loss improved from 1.28945 to 1.27902, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 470us/sample - loss: 1.2936 - val_loss: 1.2790\n",
      "Epoch 7/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2838\n",
      "Epoch 00007: val_loss improved from 1.27902 to 1.27022, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 471us/sample - loss: 1.2838 - val_loss: 1.2702\n",
      "Epoch 8/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2763\n",
      "Epoch 00008: val_loss improved from 1.27022 to 1.26418, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 96s 482us/sample - loss: 1.2763 - val_loss: 1.2642\n",
      "Epoch 9/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2695\n",
      "Epoch 00009: val_loss improved from 1.26418 to 1.25999, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 468us/sample - loss: 1.2695 - val_loss: 1.2600\n",
      "Epoch 10/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2639\n",
      "Epoch 00010: val_loss improved from 1.25999 to 1.25177, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 470us/sample - loss: 1.2639 - val_loss: 1.2518\n",
      "Epoch 11/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2586\n",
      "Epoch 00011: val_loss did not improve from 1.25177\n",
      "200000/200000 [==============================] - 94s 468us/sample - loss: 1.2586 - val_loss: 1.2565\n",
      "Epoch 12/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2544\n",
      "Epoch 00012: val_loss improved from 1.25177 to 1.24173, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 470us/sample - loss: 1.2544 - val_loss: 1.2417\n",
      "Epoch 13/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2507\n",
      "Epoch 00013: val_loss improved from 1.24173 to 1.23971, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 469us/sample - loss: 1.2507 - val_loss: 1.2397\n",
      "Epoch 14/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2476\n",
      "Epoch 00014: val_loss improved from 1.23971 to 1.23550, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 95s 474us/sample - loss: 1.2476 - val_loss: 1.2355\n",
      "Epoch 15/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2450\n",
      "Epoch 00015: val_loss improved from 1.23550 to 1.23518, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 95s 475us/sample - loss: 1.2450 - val_loss: 1.2352\n",
      "Epoch 16/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2426\n",
      "Epoch 00016: val_loss improved from 1.23518 to 1.23316, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 472us/sample - loss: 1.2426 - val_loss: 1.2332\n",
      "Epoch 17/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2407\n",
      "Epoch 00017: val_loss improved from 1.23316 to 1.23064, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 470us/sample - loss: 1.2407 - val_loss: 1.2306\n",
      "Epoch 18/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2390\n",
      "Epoch 00018: val_loss improved from 1.23064 to 1.22999, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 468us/sample - loss: 1.2390 - val_loss: 1.2300\n",
      "Epoch 19/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2374\n",
      "Epoch 00019: val_loss improved from 1.22999 to 1.22620, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 468us/sample - loss: 1.2374 - val_loss: 1.2262\n",
      "Epoch 20/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2361\n",
      "Epoch 00020: val_loss improved from 1.22620 to 1.22590, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 95s 473us/sample - loss: 1.2361 - val_loss: 1.2259\n",
      "Epoch 21/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2348\n",
      "Epoch 00021: val_loss improved from 1.22590 to 1.22493, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 96s 478us/sample - loss: 1.2347 - val_loss: 1.2249\n",
      "Epoch 22/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2334\n",
      "Epoch 00022: val_loss improved from 1.22493 to 1.22347, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 96s 480us/sample - loss: 1.2333 - val_loss: 1.2235\n",
      "Epoch 23/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2323\n",
      "Epoch 00023: val_loss improved from 1.22347 to 1.22189, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 470us/sample - loss: 1.2323 - val_loss: 1.2219\n",
      "Epoch 24/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2311\n",
      "Epoch 00024: val_loss improved from 1.22189 to 1.22023, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 471us/sample - loss: 1.2311 - val_loss: 1.2202\n",
      "Epoch 25/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2299\n",
      "Epoch 00025: val_loss improved from 1.22023 to 1.21721, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 469us/sample - loss: 1.2299 - val_loss: 1.2172\n",
      "Epoch 26/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2290\n",
      "Epoch 00026: val_loss did not improve from 1.21721\n",
      "200000/200000 [==============================] - 94s 468us/sample - loss: 1.2290 - val_loss: 1.2195\n",
      "Epoch 27/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2280\n",
      "Epoch 00027: val_loss did not improve from 1.21721\n",
      "200000/200000 [==============================] - 95s 475us/sample - loss: 1.2280 - val_loss: 1.2179\n",
      "Epoch 28/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2271\n",
      "Epoch 00028: val_loss improved from 1.21721 to 1.21577, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 95s 477us/sample - loss: 1.2271 - val_loss: 1.2158\n",
      "Epoch 29/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2264\n",
      "Epoch 00029: val_loss did not improve from 1.21577\n",
      "200000/200000 [==============================] - 94s 469us/sample - loss: 1.2264 - val_loss: 1.2168\n",
      "Epoch 30/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2257\n",
      "Epoch 00030: val_loss improved from 1.21577 to 1.21536, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 469us/sample - loss: 1.2257 - val_loss: 1.2154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2249\n",
      "Epoch 00031: val_loss did not improve from 1.21536\n",
      "200000/200000 [==============================] - 94s 468us/sample - loss: 1.2249 - val_loss: 1.2182\n",
      "Epoch 32/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2242\n",
      "Epoch 00032: val_loss improved from 1.21536 to 1.21467, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 472us/sample - loss: 1.2242 - val_loss: 1.2147\n",
      "Epoch 33/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2237\n",
      "Epoch 00033: val_loss improved from 1.21467 to 1.21318, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 95s 474us/sample - loss: 1.2237 - val_loss: 1.2132\n",
      "Epoch 34/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2228\n",
      "Epoch 00034: val_loss improved from 1.21318 to 1.21313, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 470us/sample - loss: 1.2228 - val_loss: 1.2131\n",
      "Epoch 35/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2222\n",
      "Epoch 00035: val_loss improved from 1.21313 to 1.21163, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 468us/sample - loss: 1.2222 - val_loss: 1.2116\n",
      "Epoch 36/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2218\n",
      "Epoch 00036: val_loss improved from 1.21163 to 1.20944, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 95s 477us/sample - loss: 1.2218 - val_loss: 1.2094\n",
      "Epoch 37/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2208\n",
      "Epoch 00037: val_loss did not improve from 1.20944\n",
      "200000/200000 [==============================] - 93s 467us/sample - loss: 1.2208 - val_loss: 1.2113\n",
      "Epoch 38/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2203\n",
      "Epoch 00038: val_loss improved from 1.20944 to 1.20887, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 469us/sample - loss: 1.2203 - val_loss: 1.2089\n",
      "Epoch 39/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2196\n",
      "Epoch 00039: val_loss did not improve from 1.20887\n",
      "200000/200000 [==============================] - 94s 469us/sample - loss: 1.2196 - val_loss: 1.2096\n",
      "Epoch 40/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2190\n",
      "Epoch 00040: val_loss did not improve from 1.20887\n",
      "200000/200000 [==============================] - 95s 477us/sample - loss: 1.2190 - val_loss: 1.2132\n",
      "Epoch 41/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2183\n",
      "Epoch 00041: val_loss did not improve from 1.20887\n",
      "200000/200000 [==============================] - 94s 470us/sample - loss: 1.2184 - val_loss: 1.2104\n",
      "Epoch 42/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2177\n",
      "Epoch 00042: val_loss improved from 1.20887 to 1.20685, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 95s 475us/sample - loss: 1.2177 - val_loss: 1.2069\n",
      "Epoch 43/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2172\n",
      "Epoch 00043: val_loss did not improve from 1.20685\n",
      "200000/200000 [==============================] - 94s 470us/sample - loss: 1.2172 - val_loss: 1.2105\n",
      "Epoch 44/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2167\n",
      "Epoch 00044: val_loss did not improve from 1.20685\n",
      "200000/200000 [==============================] - 94s 471us/sample - loss: 1.2167 - val_loss: 1.2125\n",
      "Epoch 45/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2160\n",
      "Epoch 00045: val_loss improved from 1.20685 to 1.20609, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 471us/sample - loss: 1.2160 - val_loss: 1.2061\n",
      "Epoch 46/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2157\n",
      "Epoch 00046: val_loss improved from 1.20609 to 1.20562, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 97s 485us/sample - loss: 1.2158 - val_loss: 1.2056\n",
      "Epoch 47/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2154\n",
      "Epoch 00047: val_loss improved from 1.20562 to 1.20423, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 95s 475us/sample - loss: 1.2154 - val_loss: 1.2042\n",
      "Epoch 48/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2148\n",
      "Epoch 00048: val_loss did not improve from 1.20423\n",
      "200000/200000 [==============================] - 95s 474us/sample - loss: 1.2148 - val_loss: 1.2044\n",
      "Epoch 49/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2145\n",
      "Epoch 00049: val_loss did not improve from 1.20423\n",
      "200000/200000 [==============================] - 94s 468us/sample - loss: 1.2145 - val_loss: 1.2052\n",
      "Epoch 50/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2141\n",
      "Epoch 00050: val_loss improved from 1.20423 to 1.20371, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 470us/sample - loss: 1.2141 - val_loss: 1.2037\n",
      "Epoch 51/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2137\n",
      "Epoch 00051: val_loss did not improve from 1.20371\n",
      "200000/200000 [==============================] - 94s 468us/sample - loss: 1.2137 - val_loss: 1.2044\n",
      "Epoch 52/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2133\n",
      "Epoch 00052: val_loss did not improve from 1.20371\n",
      "200000/200000 [==============================] - 95s 473us/sample - loss: 1.2133 - val_loss: 1.2103\n",
      "Epoch 53/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2131\n",
      "Epoch 00053: val_loss did not improve from 1.20371\n",
      "200000/200000 [==============================] - 94s 471us/sample - loss: 1.2131 - val_loss: 1.2096\n",
      "Epoch 54/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2126\n",
      "Epoch 00054: val_loss did not improve from 1.20371\n",
      "200000/200000 [==============================] - 96s 480us/sample - loss: 1.2126 - val_loss: 1.2045\n",
      "Epoch 55/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2123\n",
      "Epoch 00055: val_loss did not improve from 1.20371\n",
      "200000/200000 [==============================] - 94s 471us/sample - loss: 1.2123 - val_loss: 1.2037\n",
      "Epoch 56/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2120\n",
      "Epoch 00056: val_loss improved from 1.20371 to 1.20239, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 471us/sample - loss: 1.2120 - val_loss: 1.2024\n",
      "Epoch 57/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2117\n",
      "Epoch 00057: val_loss improved from 1.20239 to 1.20187, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 468us/sample - loss: 1.2116 - val_loss: 1.2019\n",
      "Epoch 58/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2114\n",
      "Epoch 00058: val_loss did not improve from 1.20187\n",
      "200000/200000 [==============================] - 94s 472us/sample - loss: 1.2114 - val_loss: 1.2039\n",
      "Epoch 59/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2113\n",
      "Epoch 00059: val_loss did not improve from 1.20187\n",
      "200000/200000 [==============================] - 94s 468us/sample - loss: 1.2113 - val_loss: 1.2054\n",
      "Epoch 60/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2109\n",
      "Epoch 00060: val_loss did not improve from 1.20187\n",
      "200000/200000 [==============================] - 97s 483us/sample - loss: 1.2109 - val_loss: 1.2040\n",
      "Epoch 61/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2106\n",
      "Epoch 00061: val_loss improved from 1.20187 to 1.20133, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 95s 473us/sample - loss: 1.2105 - val_loss: 1.2013\n",
      "Epoch 62/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2104\n",
      "Epoch 00062: val_loss did not improve from 1.20133\n",
      "200000/200000 [==============================] - 94s 469us/sample - loss: 1.2104 - val_loss: 1.2055\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2100\n",
      "Epoch 00063: val_loss did not improve from 1.20133\n",
      "200000/200000 [==============================] - 94s 468us/sample - loss: 1.2100 - val_loss: 1.2032\n",
      "Epoch 64/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2099\n",
      "Epoch 00064: val_loss improved from 1.20133 to 1.20009, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 95s 473us/sample - loss: 1.2099 - val_loss: 1.2001\n",
      "Epoch 65/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2096\n",
      "Epoch 00065: val_loss did not improve from 1.20009\n",
      "200000/200000 [==============================] - 95s 477us/sample - loss: 1.2096 - val_loss: 1.2028\n",
      "Epoch 66/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2093\n",
      "Epoch 00066: val_loss did not improve from 1.20009\n",
      "200000/200000 [==============================] - 97s 485us/sample - loss: 1.2093 - val_loss: 1.2004\n",
      "Epoch 67/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2092\n",
      "Epoch 00067: val_loss did not improve from 1.20009\n",
      "200000/200000 [==============================] - 94s 471us/sample - loss: 1.2092 - val_loss: 1.2049\n",
      "Epoch 68/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2089\n",
      "Epoch 00068: val_loss did not improve from 1.20009\n",
      "200000/200000 [==============================] - 94s 471us/sample - loss: 1.2089 - val_loss: 1.2020\n",
      "Epoch 69/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2088\n",
      "Epoch 00069: val_loss improved from 1.20009 to 1.19994, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 470us/sample - loss: 1.2088 - val_loss: 1.1999\n",
      "Epoch 70/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2085\n",
      "Epoch 00070: val_loss improved from 1.19994 to 1.19908, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 469us/sample - loss: 1.2085 - val_loss: 1.1991\n",
      "Epoch 71/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2081\n",
      "Epoch 00071: val_loss did not improve from 1.19908\n",
      "200000/200000 [==============================] - 94s 469us/sample - loss: 1.2081 - val_loss: 1.2026\n",
      "Epoch 72/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2079\n",
      "Epoch 00072: val_loss did not improve from 1.19908\n",
      "200000/200000 [==============================] - 96s 482us/sample - loss: 1.2079 - val_loss: 1.2034\n",
      "Epoch 73/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2080\n",
      "Epoch 00073: val_loss did not improve from 1.19908\n",
      "200000/200000 [==============================] - 94s 470us/sample - loss: 1.2080 - val_loss: 1.1992\n",
      "Epoch 74/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2075\n",
      "Epoch 00074: val_loss improved from 1.19908 to 1.19840, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 471us/sample - loss: 1.2075 - val_loss: 1.1984\n",
      "Epoch 75/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2074\n",
      "Epoch 00075: val_loss did not improve from 1.19840\n",
      "200000/200000 [==============================] - 94s 469us/sample - loss: 1.2074 - val_loss: 1.2036\n",
      "Epoch 76/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2072\n",
      "Epoch 00076: val_loss did not improve from 1.19840\n",
      "200000/200000 [==============================] - 95s 473us/sample - loss: 1.2072 - val_loss: 1.1987\n",
      "Epoch 77/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2070\n",
      "Epoch 00077: val_loss did not improve from 1.19840\n",
      "200000/200000 [==============================] - 94s 469us/sample - loss: 1.2070 - val_loss: 1.2027\n",
      "Epoch 78/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2069\n",
      "Epoch 00078: val_loss improved from 1.19840 to 1.19803, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 470us/sample - loss: 1.2069 - val_loss: 1.1980\n",
      "Epoch 79/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2066\n",
      "Epoch 00079: val_loss did not improve from 1.19803\n",
      "200000/200000 [==============================] - 95s 475us/sample - loss: 1.2066 - val_loss: 1.1997\n",
      "Epoch 80/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2065\n",
      "Epoch 00080: val_loss improved from 1.19803 to 1.19771, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 96s 478us/sample - loss: 1.2065 - val_loss: 1.1977\n",
      "Epoch 81/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2064\n",
      "Epoch 00081: val_loss did not improve from 1.19771\n",
      "200000/200000 [==============================] - 94s 469us/sample - loss: 1.2064 - val_loss: 1.2034\n",
      "Epoch 82/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2063\n",
      "Epoch 00082: val_loss improved from 1.19771 to 1.19741, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 470us/sample - loss: 1.2063 - val_loss: 1.1974\n",
      "Epoch 83/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2062\n",
      "Epoch 00083: val_loss did not improve from 1.19741\n",
      "200000/200000 [==============================] - 94s 470us/sample - loss: 1.2062 - val_loss: 1.2023\n",
      "Epoch 84/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2060\n",
      "Epoch 00084: val_loss did not improve from 1.19741\n",
      "200000/200000 [==============================] - 95s 474us/sample - loss: 1.2060 - val_loss: 1.2008\n",
      "Epoch 85/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2059\n",
      "Epoch 00085: val_loss did not improve from 1.19741\n",
      "200000/200000 [==============================] - 95s 477us/sample - loss: 1.2059 - val_loss: 1.1979\n",
      "Epoch 86/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2058\n",
      "Epoch 00086: val_loss did not improve from 1.19741\n",
      "200000/200000 [==============================] - 94s 468us/sample - loss: 1.2058 - val_loss: 1.1998\n",
      "Epoch 87/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2056\n",
      "Epoch 00087: val_loss improved from 1.19741 to 1.19721, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 470us/sample - loss: 1.2056 - val_loss: 1.1972\n",
      "Epoch 88/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2056\n",
      "Epoch 00088: val_loss did not improve from 1.19721\n",
      "200000/200000 [==============================] - 94s 471us/sample - loss: 1.2056 - val_loss: 1.2010\n",
      "Epoch 89/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2054\n",
      "Epoch 00089: val_loss did not improve from 1.19721\n",
      "200000/200000 [==============================] - 93s 467us/sample - loss: 1.2054 - val_loss: 1.1987\n",
      "Epoch 90/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2053\n",
      "Epoch 00090: val_loss did not improve from 1.19721\n",
      "200000/200000 [==============================] - 94s 471us/sample - loss: 1.2053 - val_loss: 1.1975\n",
      "Epoch 91/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2051\n",
      "Epoch 00091: val_loss did not improve from 1.19721\n",
      "200000/200000 [==============================] - 96s 480us/sample - loss: 1.2051 - val_loss: 1.1994\n",
      "Epoch 92/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2051\n",
      "Epoch 00092: val_loss did not improve from 1.19721\n",
      "200000/200000 [==============================] - 96s 481us/sample - loss: 1.2051 - val_loss: 1.2009\n",
      "Epoch 93/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2049\n",
      "Epoch 00093: val_loss improved from 1.19721 to 1.19620, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 469us/sample - loss: 1.2049 - val_loss: 1.1962\n",
      "Epoch 94/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2049\n",
      "Epoch 00094: val_loss improved from 1.19620 to 1.19600, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 469us/sample - loss: 1.2049 - val_loss: 1.1960\n",
      "Epoch 95/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2047\n",
      "Epoch 00095: val_loss improved from 1.19600 to 1.19571, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 94s 468us/sample - loss: 1.2047 - val_loss: 1.1957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2045\n",
      "Epoch 00096: val_loss did not improve from 1.19571\n",
      "200000/200000 [==============================] - 94s 472us/sample - loss: 1.2045 - val_loss: 1.1968\n",
      "Epoch 97/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2045\n",
      "Epoch 00097: val_loss did not improve from 1.19571\n",
      "200000/200000 [==============================] - 93s 467us/sample - loss: 1.2045 - val_loss: 1.1963\n",
      "Epoch 98/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2045\n",
      "Epoch 00098: val_loss did not improve from 1.19571\n",
      "200000/200000 [==============================] - 94s 471us/sample - loss: 1.2044 - val_loss: 1.1966\n",
      "Epoch 99/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2043\n",
      "Epoch 00099: val_loss did not improve from 1.19571\n",
      "200000/200000 [==============================] - 95s 477us/sample - loss: 1.2043 - val_loss: 1.2012\n",
      "Epoch 100/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2041\n",
      "Epoch 00100: val_loss did not improve from 1.19571\n",
      "200000/200000 [==============================] - 94s 471us/sample - loss: 1.2041 - val_loss: 1.1985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x7f3d70096940>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Helper.train_enc_dec(enc_dec, (tds, tds), (vds, vds),\n",
    "                     64, 100, 0.001, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.2000\n",
      "Epoch 00001: val_loss improved from inf to 1.19254, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 96s 481us/sample - loss: 1.2000 - val_loss: 1.1925\n",
      "Epoch 2/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1996\n",
      "Epoch 00002: val_loss did not improve from 1.19254\n",
      "200000/200000 [==============================] - 95s 474us/sample - loss: 1.1996 - val_loss: 1.1954\n",
      "Epoch 3/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1994\n",
      "Epoch 00003: val_loss improved from 1.19254 to 1.19200, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 97s 487us/sample - loss: 1.1994 - val_loss: 1.1920\n",
      "Epoch 4/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1993\n",
      "Epoch 00004: val_loss improved from 1.19200 to 1.19112, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 95s 477us/sample - loss: 1.1993 - val_loss: 1.1911\n",
      "Epoch 5/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1992\n",
      "Epoch 00005: val_loss did not improve from 1.19112\n",
      "200000/200000 [==============================] - 96s 478us/sample - loss: 1.1992 - val_loss: 1.1947\n",
      "Epoch 6/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1992\n",
      "Epoch 00006: val_loss did not improve from 1.19112\n",
      "200000/200000 [==============================] - 95s 475us/sample - loss: 1.1992 - val_loss: 1.1928\n",
      "Epoch 7/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1991\n",
      "Epoch 00007: val_loss did not improve from 1.19112\n",
      "200000/200000 [==============================] - 95s 474us/sample - loss: 1.1991 - val_loss: 1.1922\n",
      "Epoch 8/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1991\n",
      "Epoch 00008: val_loss did not improve from 1.19112\n",
      "200000/200000 [==============================] - 95s 474us/sample - loss: 1.1991 - val_loss: 1.1928\n",
      "Epoch 9/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1989\n",
      "Epoch 00009: val_loss did not improve from 1.19112\n",
      "200000/200000 [==============================] - 96s 480us/sample - loss: 1.1989 - val_loss: 1.1919\n",
      "Epoch 10/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1988\n",
      "Epoch 00010: val_loss did not improve from 1.19112\n",
      "200000/200000 [==============================] - 97s 487us/sample - loss: 1.1988 - val_loss: 1.1940\n",
      "Epoch 11/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1988\n",
      "Epoch 00011: val_loss did not improve from 1.19112\n",
      "200000/200000 [==============================] - 97s 483us/sample - loss: 1.1988 - val_loss: 1.1921\n",
      "Epoch 12/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1986\n",
      "Epoch 00012: val_loss did not improve from 1.19112\n",
      "200000/200000 [==============================] - 95s 475us/sample - loss: 1.1986 - val_loss: 1.1914\n",
      "Epoch 13/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1988\n",
      "Epoch 00013: val_loss improved from 1.19112 to 1.19090, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 96s 479us/sample - loss: 1.1987 - val_loss: 1.1909\n",
      "Epoch 14/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1987\n",
      "Epoch 00014: val_loss did not improve from 1.19090\n",
      "200000/200000 [==============================] - 95s 475us/sample - loss: 1.1987 - val_loss: 1.1939\n",
      "Epoch 15/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1987\n",
      "Epoch 00015: val_loss did not improve from 1.19090\n",
      "200000/200000 [==============================] - 95s 476us/sample - loss: 1.1986 - val_loss: 1.1924\n",
      "Epoch 16/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1985\n",
      "Epoch 00016: val_loss did not improve from 1.19090\n",
      "200000/200000 [==============================] - 95s 476us/sample - loss: 1.1985 - val_loss: 1.1920\n",
      "Epoch 17/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1986\n",
      "Epoch 00017: val_loss did not improve from 1.19090\n",
      "200000/200000 [==============================] - 98s 488us/sample - loss: 1.1985 - val_loss: 1.1910\n",
      "Epoch 18/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1985\n",
      "Epoch 00018: val_loss did not improve from 1.19090\n",
      "200000/200000 [==============================] - 96s 478us/sample - loss: 1.1985 - val_loss: 1.1917\n",
      "Epoch 19/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1984\n",
      "Epoch 00019: val_loss did not improve from 1.19090\n",
      "200000/200000 [==============================] - 95s 475us/sample - loss: 1.1984 - val_loss: 1.1929\n",
      "Epoch 20/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1984\n",
      "Epoch 00020: val_loss did not improve from 1.19090\n",
      "200000/200000 [==============================] - 95s 475us/sample - loss: 1.1984 - val_loss: 1.1917\n",
      "Epoch 21/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1983\n",
      "Epoch 00021: val_loss did not improve from 1.19090\n",
      "200000/200000 [==============================] - 95s 477us/sample - loss: 1.1983 - val_loss: 1.1958\n",
      "Epoch 22/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1983\n",
      "Epoch 00022: val_loss did not improve from 1.19090\n",
      "200000/200000 [==============================] - 95s 473us/sample - loss: 1.1983 - val_loss: 1.1975\n",
      "Epoch 23/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1982\n",
      "Epoch 00023: val_loss did not improve from 1.19090\n",
      "200000/200000 [==============================] - 97s 485us/sample - loss: 1.1982 - val_loss: 1.1920\n",
      "Epoch 24/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1982\n",
      "Epoch 00024: val_loss did not improve from 1.19090\n",
      "200000/200000 [==============================] - 96s 479us/sample - loss: 1.1982 - val_loss: 1.1912\n",
      "Epoch 25/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1981\n",
      "Epoch 00025: val_loss did not improve from 1.19090\n",
      "200000/200000 [==============================] - 96s 481us/sample - loss: 1.1981 - val_loss: 1.1934\n",
      "Epoch 26/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1982\n",
      "Epoch 00026: val_loss did not improve from 1.19090\n",
      "200000/200000 [==============================] - 96s 478us/sample - loss: 1.1982 - val_loss: 1.1936\n",
      "Epoch 27/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1980\n",
      "Epoch 00027: val_loss did not improve from 1.19090\n",
      "200000/200000 [==============================] - 95s 477us/sample - loss: 1.1980 - val_loss: 1.1914\n",
      "Epoch 28/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1980\n",
      "Epoch 00028: val_loss improved from 1.19090 to 1.19067, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 95s 477us/sample - loss: 1.1980 - val_loss: 1.1907\n",
      "Epoch 29/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1980\n",
      "Epoch 00029: val_loss improved from 1.19067 to 1.19002, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 98s 490us/sample - loss: 1.1980 - val_loss: 1.1900\n",
      "Epoch 30/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1979\n",
      "Epoch 00030: val_loss did not improve from 1.19002\n",
      "200000/200000 [==============================] - 96s 478us/sample - loss: 1.1979 - val_loss: 1.1918\n",
      "Epoch 31/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1979\n",
      "Epoch 00031: val_loss did not improve from 1.19002\n",
      "200000/200000 [==============================] - 95s 476us/sample - loss: 1.1979 - val_loss: 1.1910\n",
      "Epoch 32/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1979\n",
      "Epoch 00032: val_loss did not improve from 1.19002\n",
      "200000/200000 [==============================] - 95s 477us/sample - loss: 1.1979 - val_loss: 1.1907\n",
      "Epoch 33/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1979\n",
      "Epoch 00033: val_loss did not improve from 1.19002\n",
      "200000/200000 [==============================] - 96s 479us/sample - loss: 1.1979 - val_loss: 1.1910\n",
      "Epoch 34/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1977\n",
      "Epoch 00034: val_loss did not improve from 1.19002\n",
      "200000/200000 [==============================] - 96s 478us/sample - loss: 1.1978 - val_loss: 1.1955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1978\n",
      "Epoch 00035: val_loss did not improve from 1.19002\n",
      "200000/200000 [==============================] - 96s 482us/sample - loss: 1.1978 - val_loss: 1.1902\n",
      "Epoch 36/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1978\n",
      "Epoch 00036: val_loss did not improve from 1.19002\n",
      "200000/200000 [==============================] - 96s 480us/sample - loss: 1.1978 - val_loss: 1.1905\n",
      "Epoch 37/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1976\n",
      "Epoch 00037: val_loss did not improve from 1.19002\n",
      "200000/200000 [==============================] - 98s 490us/sample - loss: 1.1976 - val_loss: 1.1908\n",
      "Epoch 38/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1976\n",
      "Epoch 00038: val_loss did not improve from 1.19002\n",
      "200000/200000 [==============================] - 96s 480us/sample - loss: 1.1976 - val_loss: 1.1908\n",
      "Epoch 39/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1977\n",
      "Epoch 00039: val_loss did not improve from 1.19002\n",
      "200000/200000 [==============================] - 95s 477us/sample - loss: 1.1977 - val_loss: 1.1928\n",
      "Epoch 40/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1976\n",
      "Epoch 00040: val_loss did not improve from 1.19002\n",
      "200000/200000 [==============================] - 95s 477us/sample - loss: 1.1976 - val_loss: 1.1925\n",
      "Epoch 41/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1975\n",
      "Epoch 00041: val_loss improved from 1.19002 to 1.18995, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 97s 486us/sample - loss: 1.1975 - val_loss: 1.1899\n",
      "Epoch 42/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1975\n",
      "Epoch 00042: val_loss did not improve from 1.18995\n",
      "200000/200000 [==============================] - 95s 477us/sample - loss: 1.1975 - val_loss: 1.1905\n",
      "Epoch 43/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1974\n",
      "Epoch 00043: val_loss did not improve from 1.18995\n",
      "200000/200000 [==============================] - 96s 482us/sample - loss: 1.1974 - val_loss: 1.1929\n",
      "Epoch 44/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1975\n",
      "Epoch 00044: val_loss did not improve from 1.18995\n",
      "200000/200000 [==============================] - 95s 474us/sample - loss: 1.1975 - val_loss: 1.1918\n",
      "Epoch 45/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1974\n",
      "Epoch 00045: val_loss did not improve from 1.18995\n",
      "200000/200000 [==============================] - 95s 477us/sample - loss: 1.1974 - val_loss: 1.1938\n",
      "Epoch 46/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1974\n",
      "Epoch 00046: val_loss did not improve from 1.18995\n",
      "200000/200000 [==============================] - 95s 476us/sample - loss: 1.1974 - val_loss: 1.1915\n",
      "Epoch 47/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1974\n",
      "Epoch 00047: val_loss improved from 1.18995 to 1.18940, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 97s 487us/sample - loss: 1.1973 - val_loss: 1.1894\n",
      "Epoch 48/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1972\n",
      "Epoch 00048: val_loss did not improve from 1.18940\n",
      "200000/200000 [==============================] - 96s 479us/sample - loss: 1.1972 - val_loss: 1.1914\n",
      "Epoch 49/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1972\n",
      "Epoch 00049: val_loss did not improve from 1.18940\n",
      "200000/200000 [==============================] - 96s 479us/sample - loss: 1.1972 - val_loss: 1.1901\n",
      "Epoch 50/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1973\n",
      "Epoch 00050: val_loss did not improve from 1.18940\n",
      "200000/200000 [==============================] - 95s 475us/sample - loss: 1.1972 - val_loss: 1.1899\n",
      "Epoch 51/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1972\n",
      "Epoch 00051: val_loss did not improve from 1.18940\n",
      "200000/200000 [==============================] - 95s 477us/sample - loss: 1.1972 - val_loss: 1.1939\n",
      "Epoch 52/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1972\n",
      "Epoch 00052: val_loss did not improve from 1.18940\n",
      "200000/200000 [==============================] - 96s 478us/sample - loss: 1.1972 - val_loss: 1.1929\n",
      "Epoch 53/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1971\n",
      "Epoch 00053: val_loss did not improve from 1.18940\n",
      "200000/200000 [==============================] - 96s 478us/sample - loss: 1.1971 - val_loss: 1.1933\n",
      "Epoch 54/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1971\n",
      "Epoch 00054: val_loss did not improve from 1.18940\n",
      "200000/200000 [==============================] - 96s 481us/sample - loss: 1.1971 - val_loss: 1.1921\n",
      "Epoch 55/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1970\n",
      "Epoch 00055: val_loss did not improve from 1.18940\n",
      "200000/200000 [==============================] - 98s 492us/sample - loss: 1.1971 - val_loss: 1.1904\n",
      "Epoch 56/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1972\n",
      "Epoch 00056: val_loss did not improve from 1.18940\n",
      "200000/200000 [==============================] - 96s 479us/sample - loss: 1.1972 - val_loss: 1.1920\n",
      "Epoch 57/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1969\n",
      "Epoch 00057: val_loss did not improve from 1.18940\n",
      "200000/200000 [==============================] - 96s 478us/sample - loss: 1.1969 - val_loss: 1.1931\n",
      "Epoch 58/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1971\n",
      "Epoch 00058: val_loss did not improve from 1.18940\n",
      "200000/200000 [==============================] - 95s 476us/sample - loss: 1.1971 - val_loss: 1.1961\n",
      "Epoch 59/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1970\n",
      "Epoch 00059: val_loss did not improve from 1.18940\n",
      "200000/200000 [==============================] - 95s 476us/sample - loss: 1.1969 - val_loss: 1.1908\n",
      "Epoch 60/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1969\n",
      "Epoch 00060: val_loss did not improve from 1.18940\n",
      "200000/200000 [==============================] - 97s 484us/sample - loss: 1.1969 - val_loss: 1.1919\n",
      "Epoch 61/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1970\n",
      "Epoch 00061: val_loss improved from 1.18940 to 1.18939, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 97s 486us/sample - loss: 1.1970 - val_loss: 1.1894\n",
      "Epoch 62/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1968\n",
      "Epoch 00062: val_loss did not improve from 1.18939\n",
      "200000/200000 [==============================] - 95s 476us/sample - loss: 1.1968 - val_loss: 1.1929\n",
      "Epoch 63/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1969\n",
      "Epoch 00063: val_loss did not improve from 1.18939\n",
      "200000/200000 [==============================] - 95s 474us/sample - loss: 1.1969 - val_loss: 1.1946\n",
      "Epoch 64/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1968\n",
      "Epoch 00064: val_loss did not improve from 1.18939\n",
      "200000/200000 [==============================] - 96s 478us/sample - loss: 1.1968 - val_loss: 1.1914\n",
      "Epoch 65/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1968\n",
      "Epoch 00065: val_loss did not improve from 1.18939\n",
      "200000/200000 [==============================] - 96s 480us/sample - loss: 1.1968 - val_loss: 1.1914\n",
      "Epoch 66/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1967\n",
      "Epoch 00066: val_loss did not improve from 1.18939\n",
      "200000/200000 [==============================] - 96s 480us/sample - loss: 1.1967 - val_loss: 1.1925\n",
      "Epoch 67/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1967\n",
      "Epoch 00067: val_loss did not improve from 1.18939\n",
      "200000/200000 [==============================] - 96s 479us/sample - loss: 1.1967 - val_loss: 1.1906\n",
      "Epoch 68/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1967\n",
      "Epoch 00068: val_loss did not improve from 1.18939\n",
      "200000/200000 [==============================] - 97s 483us/sample - loss: 1.1967 - val_loss: 1.1898\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1967\n",
      "Epoch 00069: val_loss did not improve from 1.18939\n",
      "200000/200000 [==============================] - 96s 479us/sample - loss: 1.1967 - val_loss: 1.1900\n",
      "Epoch 70/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1967\n",
      "Epoch 00070: val_loss did not improve from 1.18939\n",
      "200000/200000 [==============================] - 95s 477us/sample - loss: 1.1967 - val_loss: 1.1906\n",
      "Epoch 71/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1966\n",
      "Epoch 00071: val_loss did not improve from 1.18939\n",
      "200000/200000 [==============================] - 96s 481us/sample - loss: 1.1966 - val_loss: 1.1906\n",
      "Epoch 72/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1965\n",
      "Epoch 00072: val_loss did not improve from 1.18939\n",
      "200000/200000 [==============================] - 96s 482us/sample - loss: 1.1965 - val_loss: 1.1926\n",
      "Epoch 73/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1966\n",
      "Epoch 00073: val_loss did not improve from 1.18939\n",
      "200000/200000 [==============================] - 97s 486us/sample - loss: 1.1966 - val_loss: 1.1894\n",
      "Epoch 74/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1966\n",
      "Epoch 00074: val_loss improved from 1.18939 to 1.18932, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 96s 478us/sample - loss: 1.1966 - val_loss: 1.1893\n",
      "Epoch 75/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1965\n",
      "Epoch 00075: val_loss did not improve from 1.18932\n",
      "200000/200000 [==============================] - 95s 477us/sample - loss: 1.1965 - val_loss: 1.1897\n",
      "Epoch 76/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1964\n",
      "Epoch 00076: val_loss did not improve from 1.18932\n",
      "200000/200000 [==============================] - 95s 477us/sample - loss: 1.1964 - val_loss: 1.1928\n",
      "Epoch 77/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1964\n",
      "Epoch 00077: val_loss did not improve from 1.18932\n",
      "200000/200000 [==============================] - 96s 482us/sample - loss: 1.1964 - val_loss: 1.1907\n",
      "Epoch 78/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1964\n",
      "Epoch 00078: val_loss did not improve from 1.18932\n",
      "200000/200000 [==============================] - 97s 485us/sample - loss: 1.1964 - val_loss: 1.1939\n",
      "Epoch 79/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1965\n",
      "Epoch 00079: val_loss did not improve from 1.18932\n",
      "200000/200000 [==============================] - 97s 485us/sample - loss: 1.1965 - val_loss: 1.1964\n",
      "Epoch 80/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1963\n",
      "Epoch 00080: val_loss did not improve from 1.18932\n",
      "200000/200000 [==============================] - 97s 484us/sample - loss: 1.1963 - val_loss: 1.1905\n",
      "Epoch 81/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1963\n",
      "Epoch 00081: val_loss did not improve from 1.18932\n",
      "200000/200000 [==============================] - 96s 479us/sample - loss: 1.1963 - val_loss: 1.1919\n",
      "Epoch 82/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1962\n",
      "Epoch 00082: val_loss improved from 1.18932 to 1.18893, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 96s 479us/sample - loss: 1.1963 - val_loss: 1.1889\n",
      "Epoch 83/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1962\n",
      "Epoch 00083: val_loss improved from 1.18893 to 1.18831, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 96s 480us/sample - loss: 1.1962 - val_loss: 1.1883\n",
      "Epoch 84/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1961\n",
      "Epoch 00084: val_loss did not improve from 1.18831\n",
      "200000/200000 [==============================] - 95s 477us/sample - loss: 1.1961 - val_loss: 1.1889\n",
      "Epoch 85/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1963\n",
      "Epoch 00085: val_loss did not improve from 1.18831\n",
      "200000/200000 [==============================] - 97s 487us/sample - loss: 1.1963 - val_loss: 1.1917\n",
      "Epoch 86/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1961\n",
      "Epoch 00086: val_loss did not improve from 1.18831\n",
      "200000/200000 [==============================] - 95s 474us/sample - loss: 1.1961 - val_loss: 1.1890\n",
      "Epoch 87/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1961\n",
      "Epoch 00087: val_loss did not improve from 1.18831\n",
      "200000/200000 [==============================] - 96s 481us/sample - loss: 1.1961 - val_loss: 1.1897\n",
      "Epoch 88/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1962\n",
      "Epoch 00088: val_loss improved from 1.18831 to 1.18809, saving model to Model/tmp_enc_dec.h5\n",
      "200000/200000 [==============================] - 95s 475us/sample - loss: 1.1962 - val_loss: 1.1881\n",
      "Epoch 89/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1960\n",
      "Epoch 00089: val_loss did not improve from 1.18809\n",
      "200000/200000 [==============================] - 96s 478us/sample - loss: 1.1960 - val_loss: 1.1953\n",
      "Epoch 90/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1960\n",
      "Epoch 00090: val_loss did not improve from 1.18809\n",
      "200000/200000 [==============================] - 97s 483us/sample - loss: 1.1960 - val_loss: 1.1916\n",
      "Epoch 91/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1959\n",
      "Epoch 00091: val_loss did not improve from 1.18809\n",
      "200000/200000 [==============================] - 96s 481us/sample - loss: 1.1959 - val_loss: 1.1890\n",
      "Epoch 92/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1961\n",
      "Epoch 00092: val_loss did not improve from 1.18809\n",
      "200000/200000 [==============================] - 95s 473us/sample - loss: 1.1961 - val_loss: 1.1893\n",
      "Epoch 93/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1959\n",
      "Epoch 00093: val_loss did not improve from 1.18809\n",
      "200000/200000 [==============================] - 96s 478us/sample - loss: 1.1959 - val_loss: 1.1912\n",
      "Epoch 94/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1958\n",
      "Epoch 00094: val_loss did not improve from 1.18809\n",
      "200000/200000 [==============================] - 95s 474us/sample - loss: 1.1958 - val_loss: 1.1889\n",
      "Epoch 95/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1958\n",
      "Epoch 00095: val_loss did not improve from 1.18809\n",
      "200000/200000 [==============================] - 95s 476us/sample - loss: 1.1958 - val_loss: 1.1912\n",
      "Epoch 96/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1958\n",
      "Epoch 00096: val_loss did not improve from 1.18809\n",
      "200000/200000 [==============================] - 95s 475us/sample - loss: 1.1958 - val_loss: 1.1925\n",
      "Epoch 97/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1957\n",
      "Epoch 00097: val_loss did not improve from 1.18809\n",
      "200000/200000 [==============================] - 96s 479us/sample - loss: 1.1957 - val_loss: 1.1890\n",
      "Epoch 98/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1958\n",
      "Epoch 00098: val_loss did not improve from 1.18809\n",
      "200000/200000 [==============================] - 97s 486us/sample - loss: 1.1958 - val_loss: 1.1892\n",
      "Epoch 99/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1956\n",
      "Epoch 00099: val_loss did not improve from 1.18809\n",
      "200000/200000 [==============================] - 97s 485us/sample - loss: 1.1956 - val_loss: 1.1891\n",
      "Epoch 100/100\n",
      "199872/200000 [============================>.] - ETA: 0s - loss: 1.1958\n",
      "Epoch 00100: val_loss did not improve from 1.18809\n",
      "200000/200000 [==============================] - 95s 475us/sample - loss: 1.1958 - val_loss: 1.1895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x7f3d70096940>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Helper.train_enc_dec(enc_dec, (tds, tds), (vds, vds),\n",
    "                     64, 100, 0.0007, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import get_val_batch, get_test_batch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = get_test_batch(0, 1)\n",
    "test_imgs = np.asarray([test_batch[0, 0], test_batch[0, 1], test_batch[0, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = enc_dec.predict(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = np.asarray([tds[0], tds[10], tds[20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = enc_dec.predict(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_three(imgs, predicted):\n",
    "    for i in range(3):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.imshow(imgs[i])\n",
    "        plt.subplot(2, 3, i+4)\n",
    "        plt.imshow(predicted[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_three(train_imgs, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_three(test_imgs, test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def check_mae(imgs, predicteds):\n",
    "    maes = np.zeros(imgs.shape[0] * imgs.shape[3], dtype=np.float32)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        for j in range(imgs.shape[3]):\n",
    "            maes[i * 3 + j] = mean_absolute_error(imgs[i, :, :, j], predicteds[i, :, :, j])\n",
    "    return np.average(maes[np.nonzero(maes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006854841"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_mae(train_imgs, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005823472"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_mae(test_imgs, test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE DONE! ENC_DEC 06170037\n"
     ]
    }
   ],
   "source": [
    "Helper.save_enc_dec(enc_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = Helper.load_enc_dec(\"06170037\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = loaded.predict(test_imgs)\n",
    "predicted = loaded.predict(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convert import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "enc = Helper.load_enc(\"06170037\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now 0th saved.\n",
      "now 10th saved.\n",
      "now 20th saved.\n",
      "now 30th saved.\n",
      "now 40th saved.\n",
      "now 50th saved.\n",
      "now 60th saved.\n",
      "now 70th saved.\n",
      "now 80th saved.\n",
      "now 90th saved.\n",
      "now 100th saved.\n",
      "now 110th saved.\n",
      "now 120th saved.\n",
      "now 130th saved.\n",
      "now 140th saved.\n",
      "now 150th saved.\n",
      "now 160th saved.\n",
      "now 170th saved.\n",
      "now 180th saved.\n",
      "now 190th saved.\n",
      "now 200th saved.\n",
      "now 210th saved.\n",
      "now 220th saved.\n",
      "now 230th saved.\n",
      "now 240th saved.\n",
      "now 250th saved.\n",
      "now 260th saved.\n",
      "now 270th saved.\n",
      "now 280th saved.\n",
      "now 290th saved.\n",
      "now 300th saved.\n",
      "now 310th saved.\n",
      "now 320th saved.\n",
      "now 330th saved.\n",
      "now 340th saved.\n",
      "now 350th saved.\n",
      "now 360th saved.\n",
      "now 370th saved.\n",
      "now 380th saved.\n",
      "now 390th saved.\n",
      "now 400th saved.\n",
      "now 410th saved.\n",
      "now 420th saved.\n",
      "now 430th saved.\n",
      "now 440th saved.\n",
      "now 450th saved.\n",
      "now 460th saved.\n",
      "now 470th saved.\n",
      "now 480th saved.\n",
      "now 490th saved.\n",
      "now 500th saved.\n",
      "now 510th saved.\n",
      "now 520th saved.\n",
      "now 530th saved.\n",
      "now 540th saved.\n",
      "now 550th saved.\n",
      "now 560th saved.\n",
      "now 570th saved.\n",
      "now 580th saved.\n",
      "now 590th saved.\n",
      "now 600th saved.\n",
      "now 610th saved.\n",
      "now 620th saved.\n",
      "now 630th saved.\n",
      "now 640th saved.\n",
      "now 650th saved.\n",
      "now 660th saved.\n",
      "now 670th saved.\n",
      "now 680th saved.\n",
      "now 690th saved.\n",
      "now 700th saved.\n",
      "now 710th saved.\n",
      "now 720th saved.\n",
      "now 730th saved.\n",
      "now 740th saved.\n",
      "now 750th saved.\n",
      "now 760th saved.\n",
      "now 770th saved.\n",
      "now 780th saved.\n",
      "now 790th saved.\n",
      "now 800th saved.\n",
      "now 810th saved.\n",
      "now 820th saved.\n",
      "now 830th saved.\n",
      "now 840th saved.\n",
      "now 850th saved.\n",
      "now 860th saved.\n",
      "now 870th saved.\n",
      "now 880th saved.\n",
      "now 890th saved.\n",
      "now 900th saved.\n",
      "now 910th saved.\n",
      "now 920th saved.\n",
      "now 930th saved.\n",
      "now 940th saved.\n",
      "now 950th saved.\n",
      "now 960th saved.\n",
      "now 970th saved.\n",
      "now 980th saved.\n",
      "now 990th saved.\n",
      "now 1000th saved.\n",
      "now 1010th saved.\n",
      "now 1020th saved.\n",
      "now 1030th saved.\n",
      "now 1040th saved.\n",
      "now 1050th saved.\n",
      "now 1060th saved.\n",
      "now 1070th saved.\n",
      "now 1080th saved.\n",
      "now 1090th saved.\n",
      "now 1100th saved.\n",
      "now 1110th saved.\n",
      "now 1120th saved.\n",
      "now 1130th saved.\n",
      "now 1140th saved.\n",
      "now 1150th saved.\n",
      "now 1160th saved.\n",
      "now 1170th saved.\n",
      "now 1180th saved.\n",
      "now 1190th saved.\n",
      "now 1200th saved.\n",
      "now 1210th saved.\n",
      "now 1220th saved.\n",
      "now 1230th saved.\n",
      "now 1240th saved.\n",
      "now 1250th saved.\n",
      "now 1260th saved.\n",
      "now 1270th saved.\n",
      "now 1280th saved.\n",
      "now 1290th saved.\n",
      "now 1300th saved.\n",
      "now 1310th saved.\n",
      "now 1320th saved.\n",
      "now 1330th saved.\n",
      "now 1340th saved.\n",
      "now 1350th saved.\n",
      "now 1360th saved.\n",
      "now 1370th saved.\n",
      "now 1380th saved.\n",
      "now 1390th saved.\n",
      "now 1400th saved.\n",
      "now 1410th saved.\n",
      "now 1420th saved.\n",
      "now 1430th saved.\n",
      "now 1440th saved.\n",
      "now 1450th saved.\n",
      "now 1460th saved.\n",
      "now 1470th saved.\n",
      "now 1480th saved.\n",
      "now 1490th saved.\n",
      "now 1500th saved.\n",
      "now 1510th saved.\n",
      "now 1520th saved.\n",
      "now 1530th saved.\n",
      "now 1540th saved.\n",
      "now 1550th saved.\n",
      "now 1560th saved.\n",
      "now 1570th saved.\n",
      "now 1580th saved.\n",
      "now 1590th saved.\n",
      "now 1600th saved.\n",
      "now 1610th saved.\n",
      "now 1620th saved.\n",
      "now 1630th saved.\n",
      "now 1640th saved.\n",
      "now 1650th saved.\n",
      "now 1660th saved.\n",
      "now 1670th saved.\n",
      "now 1680th saved.\n",
      "now 1690th saved.\n",
      "now 1700th saved.\n",
      "now 1710th saved.\n",
      "now 1720th saved.\n",
      "now 1730th saved.\n",
      "now 1740th saved.\n",
      "now 1750th saved.\n",
      "now 1760th saved.\n",
      "now 1770th saved.\n",
      "now 1780th saved.\n",
      "now 1790th saved.\n",
      "now 1800th saved.\n",
      "now 1810th saved.\n",
      "now 1820th saved.\n",
      "now 1830th saved.\n",
      "now 1840th saved.\n",
      "now 1850th saved.\n",
      "now 1860th saved.\n",
      "now 1870th saved.\n",
      "now 1880th saved.\n",
      "now 1890th saved.\n",
      "now 1900th saved.\n",
      "now 1910th saved.\n",
      "now 1920th saved.\n",
      "now 1930th saved.\n",
      "now 1940th saved.\n",
      "now 1950th saved.\n",
      "now 1960th saved.\n",
      "now 1970th saved.\n",
      "now 1980th saved.\n",
      "now 1990th saved.\n",
      "now 2000th saved.\n",
      "now 2010th saved.\n",
      "now 2020th saved.\n",
      "now 2030th saved.\n",
      "now 2040th saved.\n",
      "now 2050th saved.\n",
      "now 2060th saved.\n",
      "now 2070th saved.\n",
      "now 2080th saved.\n",
      "now 2090th saved.\n",
      "now 2100th saved.\n",
      "now 2110th saved.\n",
      "now 2120th saved.\n",
      "now 2130th saved.\n",
      "now 2140th saved.\n",
      "now 2150th saved.\n",
      "now 2160th saved.\n",
      "now 2170th saved.\n",
      "now 2180th saved.\n",
      "now 2190th saved.\n",
      "now 2200th saved.\n",
      "now 2210th saved.\n",
      "now 2220th saved.\n",
      "now 2230th saved.\n",
      "now 2240th saved.\n",
      "now 2250th saved.\n",
      "now 2260th saved.\n",
      "now 2270th saved.\n",
      "now 2280th saved.\n",
      "now 2290th saved.\n",
      "now 2300th saved.\n",
      "now 2310th saved.\n",
      "now 2320th saved.\n",
      "now 2330th saved.\n",
      "now 2340th saved.\n",
      "now 2350th saved.\n",
      "now 2360th saved.\n",
      "now 2370th saved.\n",
      "now 2380th saved.\n",
      "now 2390th saved.\n",
      "now 2400th saved.\n",
      "now 2410th saved.\n",
      "now 2420th saved.\n",
      "now 2430th saved.\n",
      "now 2440th saved.\n",
      "now 2450th saved.\n",
      "now 2460th saved.\n",
      "now 2470th saved.\n",
      "now 2480th saved.\n",
      "now 2490th saved.\n",
      "now 2500th saved.\n",
      "now 2510th saved.\n",
      "now 2520th saved.\n",
      "now 2530th saved.\n",
      "now 2540th saved.\n",
      "now 2550th saved.\n",
      "now 2560th saved.\n",
      "now 2570th saved.\n",
      "now 2580th saved.\n",
      "now 2590th saved.\n",
      "now 2600th saved.\n",
      "now 2610th saved.\n",
      "now 2620th saved.\n",
      "now 2630th saved.\n",
      "now 2640th saved.\n",
      "now 2650th saved.\n",
      "now 2660th saved.\n",
      "now 2670th saved.\n",
      "now 2680th saved.\n",
      "now 2690th saved.\n",
      "now 2700th saved.\n",
      "now 2710th saved.\n",
      "now 2720th saved.\n",
      "now 2730th saved.\n",
      "now 2740th saved.\n",
      "now 2750th saved.\n",
      "now 2760th saved.\n",
      "now 2770th saved.\n",
      "now 2780th saved.\n",
      "now 2790th saved.\n",
      "now 2800th saved.\n",
      "now 2810th saved.\n",
      "now 2820th saved.\n",
      "now 2830th saved.\n",
      "now 2840th saved.\n",
      "now 2850th saved.\n",
      "now 2860th saved.\n",
      "now 2870th saved.\n",
      "now 2880th saved.\n",
      "now 2890th saved.\n",
      "now 2900th saved.\n",
      "now 2910th saved.\n",
      "now 2920th saved.\n",
      "now 2930th saved.\n",
      "now 2940th saved.\n",
      "now 2950th saved.\n",
      "now 2960th saved.\n",
      "now 2970th saved.\n",
      "now 2980th saved.\n",
      "now 2990th saved.\n",
      "now 3000th saved.\n",
      "now 3010th saved.\n",
      "now 3020th saved.\n",
      "now 3030th saved.\n",
      "now 3040th saved.\n",
      "now 3050th saved.\n",
      "now 3060th saved.\n",
      "now 3070th saved.\n",
      "now 3080th saved.\n",
      "now 3090th saved.\n",
      "now 3100th saved.\n",
      "now 3110th saved.\n",
      "now 3120th saved.\n",
      "now 3130th saved.\n",
      "now 3140th saved.\n",
      "now 3150th saved.\n",
      "now 3160th saved.\n",
      "now 3170th saved.\n",
      "now 3180th saved.\n",
      "now 3190th saved.\n",
      "now 3200th saved.\n",
      "now 3210th saved.\n",
      "now 3220th saved.\n",
      "now 3230th saved.\n",
      "now 3240th saved.\n",
      "now 3250th saved.\n",
      "now 3260th saved.\n",
      "now 3270th saved.\n",
      "now 3280th saved.\n",
      "now 3290th saved.\n",
      "now 3300th saved.\n",
      "now 3310th saved.\n",
      "now 3320th saved.\n",
      "now 3330th saved.\n",
      "now 3340th saved.\n",
      "now 3350th saved.\n",
      "now 3360th saved.\n",
      "now 3370th saved.\n",
      "now 3380th saved.\n",
      "now 3390th saved.\n",
      "now 3400th saved.\n",
      "now 3410th saved.\n",
      "now 3420th saved.\n",
      "now 3430th saved.\n",
      "now 3440th saved.\n",
      "now 3450th saved.\n",
      "now 3460th saved.\n",
      "now 3470th saved.\n",
      "now 3480th saved.\n",
      "now 3490th saved.\n",
      "now 3500th saved.\n",
      "now 3510th saved.\n",
      "now 3520th saved.\n",
      "now 3530th saved.\n",
      "now 3540th saved.\n",
      "now 3550th saved.\n",
      "now 3560th saved.\n",
      "now 3570th saved.\n",
      "now 3580th saved.\n",
      "now 3590th saved.\n",
      "now 3600th saved.\n",
      "now 3610th saved.\n",
      "now 3620th saved.\n",
      "now 3630th saved.\n",
      "now 3640th saved.\n",
      "now 3650th saved.\n",
      "now 3660th saved.\n",
      "now 3670th saved.\n",
      "now 3680th saved.\n",
      "now 3690th saved.\n",
      "now 3700th saved.\n",
      "now 3710th saved.\n",
      "now 3720th saved.\n",
      "now 3730th saved.\n",
      "now 3740th saved.\n",
      "now 3750th saved.\n",
      "now 3760th saved.\n",
      "now 3770th saved.\n",
      "now 3780th saved.\n",
      "now 3790th saved.\n",
      "now 3800th saved.\n",
      "now 3810th saved.\n",
      "now 3820th saved.\n",
      "now 3830th saved.\n",
      "now 3840th saved.\n",
      "now 3850th saved.\n",
      "now 3860th saved.\n",
      "now 3870th saved.\n",
      "now 3880th saved.\n",
      "now 3890th saved.\n",
      "now 3900th saved.\n",
      "now 3910th saved.\n",
      "now 3920th saved.\n",
      "now 3930th saved.\n",
      "now 3940th saved.\n",
      "now 3950th saved.\n",
      "now 3960th saved.\n",
      "now 3970th saved.\n",
      "now 3980th saved.\n",
      "now 3990th saved.\n",
      "now 4000th saved.\n",
      "now 4010th saved.\n",
      "now 4020th saved.\n",
      "now 4030th saved.\n",
      "now 4040th saved.\n",
      "now 4050th saved.\n",
      "now 4060th saved.\n",
      "now 4070th saved.\n",
      "now 4080th saved.\n",
      "now 4090th saved.\n",
      "now 4100th saved.\n",
      "now 4110th saved.\n",
      "now 4120th saved.\n",
      "now 4130th saved.\n",
      "now 4140th saved.\n",
      "now 4150th saved.\n",
      "now 4160th saved.\n",
      "now 4170th saved.\n",
      "now 4180th saved.\n",
      "now 4190th saved.\n",
      "now 4200th saved.\n",
      "now 4210th saved.\n",
      "now 4220th saved.\n",
      "now 4230th saved.\n",
      "now 4240th saved.\n",
      "now 4250th saved.\n",
      "now 4260th saved.\n",
      "now 4270th saved.\n",
      "now 4280th saved.\n",
      "now 4290th saved.\n",
      "now 4300th saved.\n",
      "now 4310th saved.\n",
      "now 4320th saved.\n",
      "now 4330th saved.\n",
      "now 4340th saved.\n",
      "now 4350th saved.\n",
      "now 4360th saved.\n",
      "now 4370th saved.\n",
      "now 4380th saved.\n",
      "now 4390th saved.\n",
      "now 4400th saved.\n",
      "now 4410th saved.\n",
      "now 4420th saved.\n",
      "now 4430th saved.\n",
      "now 4440th saved.\n",
      "now 4450th saved.\n",
      "now 4460th saved.\n",
      "now 4470th saved.\n",
      "now 4480th saved.\n",
      "now 4490th saved.\n",
      "now 4500th saved.\n",
      "now 4510th saved.\n",
      "now 4520th saved.\n",
      "now 4530th saved.\n",
      "now 4540th saved.\n",
      "now 4550th saved.\n",
      "now 4560th saved.\n",
      "now 4570th saved.\n",
      "now 4580th saved.\n",
      "now 4590th saved.\n",
      "now 4600th saved.\n",
      "now 4610th saved.\n",
      "now 4620th saved.\n",
      "now 4630th saved.\n",
      "now 4640th saved.\n",
      "now 4650th saved.\n",
      "now 4660th saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now 4670th saved.\n",
      "now 4680th saved.\n",
      "now 4690th saved.\n",
      "now 4700th saved.\n",
      "now 4710th saved.\n",
      "now 4720th saved.\n",
      "now 4730th saved.\n",
      "now 4740th saved.\n",
      "now 4750th saved.\n",
      "now 4760th saved.\n",
      "now 4770th saved.\n",
      "now 4780th saved.\n",
      "now 4790th saved.\n",
      "now 4800th saved.\n",
      "now 4810th saved.\n",
      "now 4820th saved.\n",
      "now 4830th saved.\n",
      "now 4840th saved.\n",
      "now 4850th saved.\n",
      "now 4860th saved.\n",
      "now 4870th saved.\n",
      "now 4880th saved.\n",
      "now 4890th saved.\n",
      "now 4900th saved.\n",
      "now 4910th saved.\n",
      "now 4920th saved.\n",
      "now 4930th saved.\n",
      "now 4940th saved.\n",
      "now 4950th saved.\n",
      "now 4960th saved.\n",
      "now 4970th saved.\n",
      "now 4980th saved.\n",
      "now 4990th saved.\n",
      "now 5000th saved.\n",
      "now 5010th saved.\n",
      "now 5020th saved.\n",
      "now 5030th saved.\n",
      "now 5040th saved.\n",
      "now 5050th saved.\n",
      "now 5060th saved.\n",
      "now 5070th saved.\n",
      "now 5080th saved.\n",
      "now 5090th saved.\n",
      "now 5100th saved.\n",
      "now 5110th saved.\n",
      "now 5120th saved.\n",
      "now 5130th saved.\n",
      "now 5140th saved.\n",
      "now 5150th saved.\n",
      "now 5160th saved.\n",
      "now 5170th saved.\n",
      "now 5180th saved.\n",
      "now 5190th saved.\n",
      "now 5200th saved.\n",
      "now 5210th saved.\n",
      "now 5220th saved.\n",
      "now 5230th saved.\n",
      "now 5240th saved.\n",
      "now 5250th saved.\n",
      "now 5260th saved.\n",
      "now 5270th saved.\n",
      "now 5280th saved.\n",
      "now 5290th saved.\n",
      "now 5300th saved.\n",
      "now 5310th saved.\n",
      "now 5320th saved.\n",
      "now 5330th saved.\n",
      "now 5340th saved.\n",
      "now 5350th saved.\n",
      "now 5360th saved.\n",
      "now 5370th saved.\n",
      "now 5380th saved.\n",
      "now 5390th saved.\n",
      "now 5400th saved.\n",
      "now 5410th saved.\n",
      "now 5420th saved.\n",
      "now 5430th saved.\n",
      "now 5440th saved.\n",
      "now 5450th saved.\n",
      "now 5460th saved.\n",
      "now 5470th saved.\n",
      "now 5480th saved.\n",
      "now 5490th saved.\n",
      "now 5500th saved.\n",
      "now 5510th saved.\n",
      "now 5520th saved.\n",
      "now 5530th saved.\n",
      "now 5540th saved.\n",
      "now 5550th saved.\n",
      "now 5560th saved.\n",
      "now 5570th saved.\n",
      "now 5580th saved.\n",
      "now 5590th saved.\n",
      "now 5600th saved.\n",
      "now 5610th saved.\n",
      "now 5620th saved.\n",
      "now 5630th saved.\n",
      "now 5640th saved.\n",
      "now 5650th saved.\n",
      "now 5660th saved.\n",
      "now 5670th saved.\n",
      "now 5680th saved.\n",
      "now 5690th saved.\n",
      "now 5700th saved.\n",
      "now 5710th saved.\n",
      "now 5720th saved.\n",
      "now 5730th saved.\n",
      "now 5740th saved.\n",
      "now 5750th saved.\n",
      "now 5760th saved.\n",
      "now 5770th saved.\n",
      "now 5780th saved.\n",
      "now 5790th saved.\n",
      "now 5800th saved.\n",
      "now 5810th saved.\n",
      "now 5820th saved.\n",
      "now 5830th saved.\n",
      "now 5840th saved.\n",
      "now 5850th saved.\n",
      "now 5860th saved.\n",
      "now 5870th saved.\n",
      "now 5880th saved.\n",
      "now 5890th saved.\n",
      "now 5900th saved.\n",
      "now 5910th saved.\n",
      "now 5920th saved.\n",
      "now 5930th saved.\n",
      "now 5940th saved.\n",
      "now 5950th saved.\n",
      "now 5960th saved.\n",
      "now 5970th saved.\n",
      "now 5980th saved.\n",
      "now 5990th saved.\n",
      "now 6000th saved.\n",
      "now 6010th saved.\n",
      "now 6020th saved.\n",
      "now 6030th saved.\n",
      "now 6040th saved.\n",
      "now 6050th saved.\n",
      "now 6060th saved.\n",
      "now 6070th saved.\n",
      "now 6080th saved.\n",
      "now 6090th saved.\n",
      "now 6100th saved.\n",
      "now 6110th saved.\n",
      "now 6120th saved.\n",
      "now 6130th saved.\n",
      "now 6140th saved.\n",
      "now 6150th saved.\n",
      "now 6160th saved.\n",
      "now 6170th saved.\n",
      "now 6180th saved.\n",
      "now 6190th saved.\n",
      "now 6200th saved.\n",
      "now 6210th saved.\n",
      "now 6220th saved.\n",
      "now 6230th saved.\n",
      "now 6240th saved.\n",
      "now 6250th saved.\n",
      "now 6260th saved.\n",
      "now 6270th saved.\n",
      "now 6280th saved.\n",
      "now 6290th saved.\n",
      "now 6300th saved.\n",
      "now 6310th saved.\n",
      "now 6320th saved.\n",
      "now 6330th saved.\n",
      "now 6340th saved.\n",
      "now 6350th saved.\n",
      "now 6360th saved.\n",
      "now 6370th saved.\n",
      "now 6380th saved.\n",
      "now 6390th saved.\n",
      "now 6400th saved.\n",
      "now 6410th saved.\n",
      "now 6420th saved.\n",
      "now 6430th saved.\n",
      "now 6440th saved.\n",
      "now 6450th saved.\n",
      "now 6460th saved.\n",
      "now 6470th saved.\n",
      "now 6480th saved.\n",
      "now 6490th saved.\n",
      "now 6500th saved.\n",
      "now 6510th saved.\n",
      "now 6520th saved.\n",
      "now 6530th saved.\n",
      "now 6540th saved.\n",
      "now 6550th saved.\n",
      "now 6560th saved.\n",
      "now 6570th saved.\n",
      "now 6580th saved.\n",
      "now 6590th saved.\n",
      "now 6600th saved.\n",
      "now 6610th saved.\n",
      "now 6620th saved.\n",
      "now 6630th saved.\n",
      "now 6640th saved.\n",
      "now 6650th saved.\n",
      "now 6660th saved.\n",
      "now 6670th saved.\n",
      "now 6680th saved.\n",
      "now 6690th saved.\n",
      "now 6700th saved.\n",
      "now 6710th saved.\n",
      "now 6720th saved.\n",
      "now 6730th saved.\n",
      "now 6740th saved.\n",
      "now 6750th saved.\n",
      "now 6760th saved.\n",
      "now 6770th saved.\n",
      "now 6780th saved.\n",
      "now 6790th saved.\n",
      "now 6800th saved.\n",
      "now 6810th saved.\n",
      "now 6820th saved.\n",
      "now 6830th saved.\n",
      "now 6840th saved.\n",
      "now 6850th saved.\n",
      "now 6860th saved.\n",
      "now 6870th saved.\n",
      "now 6880th saved.\n",
      "now 6890th saved.\n",
      "now 6900th saved.\n",
      "now 6910th saved.\n",
      "now 6920th saved.\n",
      "now 6930th saved.\n",
      "now 6940th saved.\n",
      "now 6950th saved.\n",
      "now 6960th saved.\n",
      "now 6970th saved.\n",
      "now 6980th saved.\n",
      "now 6990th saved.\n",
      "now 7000th saved.\n",
      "now 7010th saved.\n",
      "now 7020th saved.\n",
      "now 7030th saved.\n",
      "now 7040th saved.\n",
      "now 7050th saved.\n",
      "now 7060th saved.\n",
      "now 7070th saved.\n",
      "now 7080th saved.\n",
      "now 7090th saved.\n",
      "now 7100th saved.\n",
      "now 7110th saved.\n",
      "now 7120th saved.\n",
      "now 7130th saved.\n",
      "now 7140th saved.\n",
      "now 7150th saved.\n",
      "now 7160th saved.\n",
      "now 7170th saved.\n",
      "now 7180th saved.\n",
      "now 7190th saved.\n",
      "now 7200th saved.\n",
      "now 7210th saved.\n",
      "now 7220th saved.\n",
      "now 7230th saved.\n",
      "now 7240th saved.\n",
      "now 7250th saved.\n",
      "now 7260th saved.\n",
      "now 7270th saved.\n",
      "now 7280th saved.\n",
      "now 7290th saved.\n",
      "now 7300th saved.\n",
      "now 7310th saved.\n",
      "now 7320th saved.\n",
      "now 7330th saved.\n",
      "now 7340th saved.\n",
      "now 7350th saved.\n",
      "now 7360th saved.\n",
      "now 7370th saved.\n",
      "now 7380th saved.\n",
      "now 7390th saved.\n",
      "now 7400th saved.\n",
      "now 7410th saved.\n",
      "now 7420th saved.\n",
      "now 7430th saved.\n",
      "now 7440th saved.\n",
      "now 7450th saved.\n",
      "now 7460th saved.\n",
      "now 7470th saved.\n",
      "now 7480th saved.\n",
      "now 7490th saved.\n",
      "now 7500th saved.\n",
      "now 7510th saved.\n",
      "now 7520th saved.\n",
      "now 7530th saved.\n",
      "now 7540th saved.\n",
      "now 7550th saved.\n",
      "now 7560th saved.\n",
      "now 7570th saved.\n",
      "now 7580th saved.\n",
      "now 7590th saved.\n",
      "now 7600th saved.\n",
      "now 7610th saved.\n",
      "now 7620th saved.\n",
      "now 7630th saved.\n",
      "now 7640th saved.\n",
      "now 7650th saved.\n",
      "now 7660th saved.\n",
      "now 7670th saved.\n",
      "now 7680th saved.\n",
      "now 7690th saved.\n",
      "now 7700th saved.\n",
      "now 7710th saved.\n",
      "now 7720th saved.\n",
      "now 7730th saved.\n",
      "now 7740th saved.\n",
      "now 7750th saved.\n",
      "now 7760th saved.\n",
      "now 7770th saved.\n",
      "now 7780th saved.\n",
      "now 7790th saved.\n",
      "now 7800th saved.\n",
      "now 7810th saved.\n",
      "now 7820th saved.\n",
      "now 7830th saved.\n",
      "now 7840th saved.\n",
      "now 7850th saved.\n",
      "now 7860th saved.\n",
      "now 7870th saved.\n",
      "now 7880th saved.\n",
      "now 7890th saved.\n",
      "now 7900th saved.\n",
      "now 7910th saved.\n",
      "now 7920th saved.\n",
      "now 7930th saved.\n",
      "now 7940th saved.\n",
      "now 7950th saved.\n",
      "now 7960th saved.\n",
      "now 7970th saved.\n",
      "now 7980th saved.\n",
      "now 7990th saved.\n",
      "now 8000th saved.\n",
      "now 8010th saved.\n",
      "now 8020th saved.\n",
      "now 8030th saved.\n",
      "now 8040th saved.\n",
      "now 8050th saved.\n",
      "now 8060th saved.\n",
      "now 8070th saved.\n",
      "now 8080th saved.\n",
      "now 8090th saved.\n",
      "now 8100th saved.\n",
      "now 8110th saved.\n",
      "now 8120th saved.\n",
      "now 8130th saved.\n",
      "now 8140th saved.\n",
      "now 8150th saved.\n",
      "now 8160th saved.\n",
      "now 8170th saved.\n",
      "now 8180th saved.\n",
      "now 8190th saved.\n",
      "now 8200th saved.\n",
      "now 8210th saved.\n",
      "now 8220th saved.\n",
      "now 8230th saved.\n",
      "now 8240th saved.\n",
      "now 8250th saved.\n",
      "now 8260th saved.\n",
      "now 8270th saved.\n",
      "now 8280th saved.\n",
      "now 8290th saved.\n",
      "now 8300th saved.\n",
      "now 8310th saved.\n",
      "now 8320th saved.\n",
      "now 8330th saved.\n",
      "now 8340th saved.\n",
      "now 8350th saved.\n",
      "now 8360th saved.\n",
      "now 8370th saved.\n",
      "now 8380th saved.\n",
      "now 8390th saved.\n",
      "now 8400th saved.\n",
      "now 8410th saved.\n",
      "now 8420th saved.\n",
      "now 8430th saved.\n",
      "now 8440th saved.\n",
      "now 8450th saved.\n",
      "now 8460th saved.\n",
      "now 8470th saved.\n",
      "now 8480th saved.\n",
      "now 8490th saved.\n",
      "now 8500th saved.\n",
      "now 8510th saved.\n",
      "now 8520th saved.\n",
      "now 8530th saved.\n",
      "now 8540th saved.\n",
      "now 8550th saved.\n",
      "now 8560th saved.\n",
      "now 8570th saved.\n",
      "now 8580th saved.\n",
      "now 8590th saved.\n",
      "now 8600th saved.\n",
      "now 8610th saved.\n",
      "now 8620th saved.\n",
      "now 8630th saved.\n",
      "now 8640th saved.\n",
      "now 8650th saved.\n",
      "now 8660th saved.\n",
      "now 8670th saved.\n",
      "now 8680th saved.\n",
      "now 8690th saved.\n",
      "now 8700th saved.\n",
      "now 8710th saved.\n",
      "now 8720th saved.\n",
      "now 8730th saved.\n",
      "now 8740th saved.\n",
      "now 8750th saved.\n",
      "now 8760th saved.\n",
      "now 8770th saved.\n",
      "now 8780th saved.\n",
      "now 8790th saved.\n",
      "now 8800th saved.\n",
      "now 8810th saved.\n",
      "now 8820th saved.\n",
      "now 8830th saved.\n",
      "now 8840th saved.\n",
      "now 8850th saved.\n",
      "now 8860th saved.\n",
      "now 8870th saved.\n",
      "now 8880th saved.\n",
      "now 8890th saved.\n",
      "now 8900th saved.\n",
      "now 8910th saved.\n",
      "now 8920th saved.\n",
      "now 8930th saved.\n",
      "now 8940th saved.\n",
      "now 8950th saved.\n",
      "now 8960th saved.\n",
      "now 8970th saved.\n",
      "now 8980th saved.\n",
      "now 8990th saved.\n",
      "now 9000th saved.\n",
      "now 9010th saved.\n",
      "now 9020th saved.\n",
      "now 9030th saved.\n",
      "now 9040th saved.\n",
      "now 9050th saved.\n",
      "now 9060th saved.\n",
      "now 9070th saved.\n",
      "now 9080th saved.\n",
      "now 9090th saved.\n",
      "now 9100th saved.\n",
      "now 9110th saved.\n",
      "now 9120th saved.\n",
      "now 9130th saved.\n",
      "now 9140th saved.\n",
      "now 9150th saved.\n",
      "now 9160th saved.\n",
      "now 9170th saved.\n",
      "now 9180th saved.\n",
      "now 9190th saved.\n",
      "now 9200th saved.\n",
      "now 9210th saved.\n",
      "now 9220th saved.\n",
      "now 9230th saved.\n",
      "now 9240th saved.\n",
      "now 9250th saved.\n",
      "now 9260th saved.\n",
      "now 9270th saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now 9280th saved.\n",
      "now 9290th saved.\n",
      "now 9300th saved.\n",
      "now 9310th saved.\n",
      "now 9320th saved.\n",
      "now 9330th saved.\n",
      "now 9340th saved.\n",
      "now 9350th saved.\n",
      "now 9360th saved.\n",
      "now 9370th saved.\n",
      "now 9380th saved.\n",
      "now 9390th saved.\n",
      "now 9400th saved.\n",
      "now 9410th saved.\n",
      "now 9420th saved.\n",
      "now 9430th saved.\n",
      "now 9440th saved.\n",
      "now 9450th saved.\n",
      "now 9460th saved.\n",
      "now 9470th saved.\n",
      "now 9480th saved.\n",
      "now 9490th saved.\n",
      "now 9500th saved.\n",
      "now 9510th saved.\n",
      "now 9520th saved.\n",
      "now 9530th saved.\n",
      "now 9540th saved.\n",
      "now 9550th saved.\n",
      "now 9560th saved.\n",
      "now 9570th saved.\n",
      "now 9580th saved.\n",
      "now 9590th saved.\n",
      "now 9600th saved.\n",
      "now 9610th saved.\n",
      "now 9620th saved.\n",
      "now 9630th saved.\n",
      "now 9640th saved.\n",
      "now 9650th saved.\n",
      "now 9660th saved.\n",
      "now 9670th saved.\n",
      "now 9680th saved.\n",
      "now 9690th saved.\n",
      "now 9700th saved.\n",
      "now 9710th saved.\n",
      "now 9720th saved.\n",
      "now 9730th saved.\n",
      "now 9740th saved.\n",
      "now 9750th saved.\n",
      "now 9760th saved.\n",
      "now 9770th saved.\n",
      "now 9780th saved.\n",
      "now 9790th saved.\n",
      "now 9800th saved.\n",
      "now 9810th saved.\n",
      "now 9820th saved.\n",
      "now 9830th saved.\n",
      "now 9840th saved.\n",
      "now 9850th saved.\n",
      "now 9860th saved.\n",
      "now 9870th saved.\n",
      "now 9880th saved.\n",
      "now 9890th saved.\n",
      "now 9900th saved.\n",
      "now 9910th saved.\n",
      "now 9920th saved.\n",
      "now 9930th saved.\n",
      "now 9940th saved.\n",
      "now 9950th saved.\n",
      "now 9960th saved.\n",
      "now 9970th saved.\n",
      "now 9980th saved.\n",
      "now 9990th saved.\n",
      "now 0th saved.\n",
      "now 10th saved.\n",
      "now 20th saved.\n",
      "now 30th saved.\n",
      "now 40th saved.\n",
      "now 50th saved.\n",
      "now 60th saved.\n",
      "now 70th saved.\n",
      "now 80th saved.\n",
      "now 90th saved.\n",
      "now 100th saved.\n",
      "now 110th saved.\n",
      "now 120th saved.\n",
      "now 130th saved.\n",
      "now 140th saved.\n",
      "now 150th saved.\n",
      "now 160th saved.\n",
      "now 170th saved.\n",
      "now 180th saved.\n",
      "now 190th saved.\n",
      "now 200th saved.\n",
      "now 210th saved.\n",
      "now 220th saved.\n",
      "now 230th saved.\n",
      "now 240th saved.\n",
      "now 250th saved.\n",
      "now 260th saved.\n",
      "now 270th saved.\n",
      "now 280th saved.\n",
      "now 290th saved.\n",
      "now 300th saved.\n",
      "now 310th saved.\n",
      "now 320th saved.\n",
      "now 330th saved.\n",
      "now 340th saved.\n",
      "now 350th saved.\n",
      "now 360th saved.\n",
      "now 370th saved.\n",
      "now 380th saved.\n",
      "now 390th saved.\n",
      "now 400th saved.\n",
      "now 410th saved.\n",
      "now 420th saved.\n",
      "now 430th saved.\n",
      "now 440th saved.\n",
      "now 450th saved.\n",
      "now 460th saved.\n",
      "now 470th saved.\n",
      "now 480th saved.\n",
      "now 490th saved.\n"
     ]
    }
   ],
   "source": [
    "img2vec(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factory import ModelFactory\n",
    "from loader import Loader\n",
    "from helper import Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Loader.load_vecs(\"train\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = Loader.load_vecs(\"val\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 10, 1024)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1024)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm test\n",
    "lstm = ModelFactory.get_lstm(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7f85a4c70b00>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      " 98496/100000 [============================>.] - ETA: 2s - loss: 0.1139"
     ]
    }
   ],
   "source": [
    "Helper.train_lstm(lstm, train_data, val_data, batch_size=64, epochs=10, lr=0.001, decay=0.0,\n",
    "                  loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = Helper.load_dec(\"06170037\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_in = val_data[0]\n",
    "val_out = val_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1024)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_in[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = lstm.predict(val_in[0].reshape([-1, 10, 1024]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1024)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_img = dec.predict(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8876fea940>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(predicted_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = Helper.load_dec(\"06170037\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10, 1024)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vectors = val_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1024)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = dec.predict(target_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_img = imgs[0]\n",
    "target_vec = target_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6608045c50>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEYRJREFUeJzt3VusHdV9x/HvLzbmDsZAHAcbMMINJW0xyOJSaEtIQU4ahTwgRBJVVoXqFyoRNVUCrdQSqZHKSwgPLZIVaPyQBsjViFZNXBdURWoNpjaJjTE4xAQTX7iZSwgXw78Pe075e+FzzvY5M3vv4/X7SEfnP3vNOftv7/OfWTOzZo0iAjOryweGnYCZDZ4L36xCLnyzCrnwzSrkwjerkAvfrEIufLMKTavwJS2XtE3Sdkk3tZWUmXVLUx3AI2kW8ARwJbATeBj4bEQ81l56ZtaF2dP42QuB7RHxFICku4GrgXELX5KHCZp1LCI02TrT6eqfBjyTlnc2r5nZiJvOHr8vklYCK7t+HzPr33QK/1lgUVpe2Lx2gIhYBawCd/XNRsV0uvoPA0skLZY0B7gOuK+dtMysS1Pe40fEfkl/AfwImAXcFRFbWsvMzDoz5ct5U3ozd/XNOtf1WX0zm6Fc+GYVcuGbVciFb1YhF75ZhVz4ZhVy4ZtVyIVvViEXvlmFXPhmFXLhm1XIhW9WIRe+WYVc+GYVcuGbVciFb1YhF75ZhVz4ZhVy4ZtVyIVvViEXvlmFXPhmFXLhm1XIhW9WIRe+WYUmLXxJd0naK2lzem2epLWSnmy+n9RtmmbWpn72+N8Elhev3QSsi4glwLpm2cxmiEkLPyL+C3ixePlqYHUTrwY+03JeZtahqR7jz4+IXU28G5jfUj5mNgBTfkz2mIiIiZ6CK2klsHK672Nm7ZnqHn+PpAUAzfe9460YEasiYllELJvie9XjA+lL6cusZVMt/PuAFU28AljTTjpmNgiKGLeX3ltB+jZwOXAKsAf4O+CHwL3A6cDTwLURUZ4APNjvmvjNapc3wzFObDaJiJi0nzhp4bfJhT8JF761oJ/Cn/bJPTtEubjnFm15+eUU7yvWe6fVjKxCHrJrViEXvlmF3NUftGNTfF3RdnWK/zXFdxTruatv0+Q9vlmFXPhmFXLhm1XIx/hdK6+onpriTxRtv5viPRP8DrNp8h7frEIufLMKuavftbKbflqKF02w7gspntVqRmbe45vVyIVvViF39btWblrzJGXHFW1HpPjIbtIxA+/xzarkwjerkAvfrEI+xu9auWnNk22Ul/r2p/j5CdYzmybv8c0q5MI3q5C7+l2bUywvTnF5ye7dFOfJNtzVt5Z5j29WIRe+WYVc+GYV8jF+104sli9I8VFF21spzg/R8DG+tWzSPb6kRZIekPSYpC2SbmxenydpraQnm+8ndZ+umbWhn67+fuCLEXEucDFwg6RzgZuAdRGxBFjXLJvZDDBpVz8idgG7mvhVSVvpTSdxNb2HaQKsBh4EvtxJljNN3px+qGjLc+6VXfg3U5wfm+Vn51nLDunknqQzgfOB9cD8ZqMAsJsDbzg1sxHW98k9SccB3wO+EBGvSO/triIixnsSrqSVwMrpJmpm7elrjy/pCHpF/62I+H7z8h5JC5r2BcDeg/1sRKyKiGURsayNhM1s+ibd46u3a78T2BoRX0tN9wErgH9ovq/pJMOZKG9OTy/ajk/x/qItbzp3tJmQ2YH66epfCvwp8DNJm5rX/ppewd8r6XrgaeDablI0s7b1c1b/J4w/hOTj7aZjZoPgkXtdyJNm/k7RdnSKy8dd707x84wvb4Z9qc+mwGP1zSrkwjerkLv6XTgmxR8p2vLkG68VbRtTnJ+WW579b1l5AieneHKKXynW+3WK38VmEu/xzSrkwjerkAvfrEI+xm9DeZCcZyY4rWjL65aDnP87xS+luLzs1/IlvHLOz99P8UUp/kmx3iMpfr3VjKxr3uObVciFb1Yhd/XbUG4+z0nxoqItd/W3FW2/TPFvUlx29VuQBxf+dtF2XYoXpnhfsd4mbKbyHt+sQi58swq58M0q5GP8NhxRLF+c4uOLtjz89hdFWx6mO94c+1NUbuGPS/FlRdtHU5xHFe8q1nsLm6m8xzerkAvfrELu6rfh6GI5T74xq2h7OcVbirY8/K3l293KLXweUHhR0Zb/ORtSvLFY7+3pJmVD4z2+WYVc+GYVcle/DccWy/mZQuUNPHkuvSeLttx37vhGnKUpXlC05fuD8n1D5T1Fnnxj5vIe36xCLnyzCrnwzSrkY/ypysfu5ci9vDktr3k9k+IXirYO58ifUyzPTXE552dezpfw3sQOF5Pu8SUdJekhSY9K2iLpK83riyWtl7Rd0j2Syr8tMxtR/XT13wSuiIjz6J0MXi7pYuBW4LaIOJveieDru0vTzNrUz7Pzgvd6f0c0XwFcAXyueX01cAtwR/spzgDlJbvcJy4no/tVit8o2jrs6pcf9CkpLrv6D6Q4H5n48t3ho6+Te5JmNU/K3QusBX4O7IuIsXvNdvL+aSXNbET1VfgR8U5ELKU3E9OFHDi51IQkrZS0QdKGydc2s0E4pMt5EbGPXk/wEmCupLEe5ELg2XF+ZlVELIuIZdPK1MxaM+kxvqRTgbcjYp+ko4Er6Z3YewC4BrgbWAGs6TLRkZOPx8tj9edSfFzR1vEkmlk+9XBC0ZaXy6G4j6fYk20cnvq5jr8AWC1pFr0ewr0Rcb+kx4C7Jf09vcu9d3aYp5m1qJ+z+j8Fzj/I60/RO943sxnGI/faUHb18+1t5bz6ec69jq+P5RM4Hyra8pO89xRt+fJeh1cYbYg8Vt+sQi58swq5q9+Gic7qP1e05X51OR9fPg3fQh873zxxVtGWP/jy0VivtpuGjSDv8c0q5MI3q5AL36xCPsZvQznZxr+leHfRtjPF5V19eTM8xUt9443W+2ixXp47pEw/L5cp2uHBe3yzCrnwzSrkrn4byr7yQyku587PN+a8OkHbFOUPNE+2cfIEb1Vezsv/nImOPnypb+byHt+sQi58swq58M0q5GP8LuTJNstb37IODpLzMN3fSvGHi/UmGjmc79w7KsXlvPr5PIGP/2cW7/HNKuTCN6uQu/pd67jPW26582i9D6b4mGK9/HPltID5MmDuzpdd/bz8mwnaPB//6PEe36xCLnyzCrmrP8OVXfilKf6DFJ9arPdyistpAfPIvfxA32OL9fLcfOVDFfIU3a+k2Gf7R4P3+GYVcuGbVciFb1YhH+PPQHlrPb9ouyrFZ6R4otF584q2PFovP0KrvOx3fIrLS33/lOJNKfYx/mjoe4/fPCp7o6T7m+XFktZL2i7pHklzJvsdZjYaDqWrfyOwNS3fCtwWEWfTe3bM9W0mZmbd6aurL2kh8CfAV4G/lCTgCuBzzSqrgVuAOzrI0Qq5a/WRoi3fjJOf1vVasd4TKd5atB2Z4tdTvLRYb7z3ggMvET6KjZp+9/hfB77Ee6MvTwb2RcTY570TOK3l3MysI5MWvqRPAXsj4pGpvIGklZI2SNowlZ83s/b109W/FPi0pE/SO+F7AnA7MFfS7Gavv5D3D94CICJWAasAJPmkrtkImLTwI+Jm4GYASZcDfxURn5f0HeAa4G5gBbCmwzwtycfg5QeYJ87Ml+xeLtbbkeLHi7a8br6z7q1ivTnjrAcHPjLQc/OPnukM4PkyvRN92+kd89/ZTkpm1rVDGsATEQ8CDzbxU8CF7adkZl3zyL0ZKF86e7po25jiXSk+sljvVykun+Sd78jL3fTyvc5JcXkYkH+nT+yMHo/VN6uQC9+sQu7qz0BvpLh8Qlc+q59Hz51RrJcf2ls+uSt3zfNTdU8o1suj+rYVbc+n2HPujR7v8c0q5MI3q5AL36xCPsafgfIx+RtF294Uvz5ODAdOvlHe4ZfnyM9z859ZrJdH+JWX+vLv8OW80eM9vlmFXPhmFXJXf4YrL5Xlrv/bE6x3Yor/uGhbnOKjU1w++Pe+FD9VtJUj+Wy0eI9vViEXvlmFXPhmFfIx/mEmXzrLd/GVE3H8MsX7irY8r36+DPg/xXrrU/xC0eZhuqPNe3yzCrnwzSqkiMGNq/Jkm6MjH+N9uGj7oxTnEXgbi/Xy7KrlCEIbnoiYdJpD7/HNKuTCN6uQu/r2vq1/njY7P2X37WK9vOwPdnS4q29mB+XCN6uQC9+sQh65ZxPe4WeHp74KX9IO4FV6k7/sj4hlkuYB99CbmGUHcG1EvNRNmmbWpkPp6n8sIpZGxLJm+SZgXUQsAdY1y2Y2A0znGP9qYHUTrwY+M/10zGwQ+i38AH4s6RFJK5vX5kfE2OPZdgPzW8/OzDrR78m9yyLiWUkfBNZKOuCR6hER4w3OaTYUKw/WZmbDccgj9yTdArwG/DlweUTskrQAeDAiypmay5/1AC+zjrUyck/SsZKOH4uBq4DN9OZaXNGstgJYM/VUzWyQJt3jSzoL+EGzOBv4l4j4qqSTgXuB0+k9T+HaiHhxkt/lPb5Zx/rZ4/smHbPDjG/SMbODcuGbVciFb1YhF75ZhVz4ZhVy4ZtVyIVvViEXvlmFXPhmFXLhm1XIhW9WIRe+WYVc+GYVcuGbVciFb1YhF75ZhVz4ZhVy4ZtVyIVvViEXvlmFXPhmFXLhm1XIhW9WIRe+WYVc+GYV6qvwJc2V9F1Jj0vaKukSSfMkrZX0ZPP9pK6TNbN29LvHvx3494g4BzgP2ArcBKyLiCXAumbZzGaAfh6aeSKwCTgr0sqStuHHZJuNnLaenbcYeA74Z0kbJX2jeVz2/IjY1ayzG5g/9VTNbJD6KfzZwAXAHRFxPvBrim590xM46N5c0kpJGyRtmG6yZtaOfgp/J7AzItY3y9+ltyHY03Txab7vPdgPR8SqiFgWEcvaSNjMpm/Swo+I3cAzksaO3z8OPAbcB6xoXlsBrOkkQzNr3aQn9wAkLQW+AcwBngL+jN5G417gdOBp4NqIeHGS3+OTe2Yd6+fkXl+F3xYXvln32jqrb2aHGRe+WYVc+GYVcuGbVciFb1YhF75ZhVz4ZhWaPeD3e57eYJ9TmniYRiEHcB4l53GgQ83jjH5WGugAnv9/U2nDsMfuj0IOzsN5DCsPd/XNKuTCN6vQsAp/1ZDeNxuFHMB5lJzHgTrJYyjH+GY2XO7qm1VooIUvabmkbZK2SxrYrLyS7pK0V9Lm9NrApweXtEjSA5Iek7RF0o3DyEXSUZIekvRok8dXmtcXS1rffD73SJrTZR4pn1nNfI73DysPSTsk/UzSprFp4ob0NzKQqewHVviSZgH/CHwCOBf4rKRzB/T23wSWF68NY3rw/cAXI+Jc4GLghub/YNC5vAlcERHnAUuB5ZIuBm4FbouIs4GXgOs7zmPMjfSmbB8zrDw+FhFL0+WzYfyNDGYq+4gYyBdwCfCjtHwzcPMA3/9MYHNa3gYsaOIFwLZB5ZJyWANcOcxcgGOA/wUuojdQZPbBPq8O339h88d8BXA/oCHlsQM4pXhtoJ8LcCLwC5pzb13mMciu/mnAM2l5Z/PasAx1enBJZwLnA+uHkUvTvd5Eb5LUtcDPgX0Rsb9ZZVCfz9eBLwHvNssnDymPAH4s6RFJK5vXBv25DGwqe5/cY+Lpwbsg6Tjge8AXIuKVYeQSEe9ExFJ6e9wLgXO6fs+SpE8BeyPikUG/90FcFhEX0DsUvUHSH+bGAX0u05rK/lAMsvCfBRal5YXNa8PS1/TgbZN0BL2i/1ZEfH+YuQBExD7gAXpd6rmSxu7fGMTncynwaUk7gLvpdfdvH0IeRMSzzfe9wA/obQwH/blMayr7QzHIwn8YWNKcsZ0DXEdviu5hGfj04JIE3AlsjYivDSsXSadKmtvER9M7z7CV3gbgmkHlERE3R8TCiDiT3t/Df0bE5wedh6RjJR0/FgNXAZsZ8OcSg5zKvuuTJsVJik8CT9A7nvybAb7vt4FdwNv0tqrX0zuWXAc8CfwHMG8AeVxGr5v2U3rPI9zU/J8MNBfg94CNTR6bgb9tXj8LeAjYDnwHOHKAn9HlwP3DyKN5v0ebry1jf5tD+htZCmxoPpsfAid1kYdH7plVyCf3zCrkwjerkAvfrEIufLMKufDNKuTCN6uQC9+sQi58swr9H4JBPgS9SBA8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89488047, 1.23381825, 1.88626546, ..., 0.99039492, 1.11052763,\n",
       "       1.25520724])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_vec + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = dec.predict(target_vector - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f65f08bb400>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
